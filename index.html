<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V3LHXHHFY5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V3LHXHHFY5');
</script>
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Radhika Dua</title>

  <meta name="author" content="Radhika Dua">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

  <style>
  ::-webkit-scrollbar {
    width: 8px; /* Width of the entire scrollbar */
  }

  ::-webkit-scrollbar-track {
    background-color: #f1f1f1; /* Color of the tracking area */
  }

  ::-webkit-scrollbar-thumb {
    background-color: #888; /* Color of the scroll thumb */
    border-radius: 20px; /* Roundness of the scroll thumb */
  }
</style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Radhika Dua</name>
              </p>
              <p>Hi, I am a Predoctoral Researcher at <a href="https://research.google/teams/india-research-lab/">Google Research India</a>, where I am working under the guidance of <a href="https://in.linkedin.com/in/gauagg">Dr. Gaurav Aggarwal</a>. Before joining Google, I was a Master's student in Graduate School of AI at Korea Advanced Institute of Science and Technology (KAIST), where I was fortunate to have <a href="https://mp2893.com/">Prof. Edward Choi</a> as my advisor. 
                My research interest lies at the intersection of Computer Vision, Machine Learning, Healthcare, and NLP. I currently work on enhancing the reliability of deep neural networks against natural and synthetic distribution shifts.
                <!-- I am interested in Artificial Intelligence at the intersection of Computer Vision, Machine Learning & Healthcare. I am currently working on out-of-distribution detection in deep learning. -->
              <br>
              <br>
              Prior to my time at KAIST, I was a Summer Intern at <a href="https://www.brown.edu/">Brown University</a>, where I was fortunate to be advised by <a href="https://ai.stanford.edu/~ssrinath/">Prof. Srinath Sridhar</a>. I also spent some time as a Visiting Researcher at <a href="https://iith.ac.in/">IIT Hyderabad</a>, where I was blessed to be supervised by <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>. 
              <p style="text-align:center">
                <a href="mailto:radhikadua@google.com">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=SeBAKsUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Radhikadua123">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/dua_radhika?lang=en">Twitter</a>
              </p>
            </td>
            <td style="width:100%;max-width:100%">
              <a href="data/rd4.jpeg"><img style="border-radius:100%;width:90%;height:37%;max-width:100%" alt="profile photo" src="data/rd4.jpeg" class="hoverZoomLink"></a>
              <!-- <a href="data/myAvatar.png"><img style="border-radius:48%;width:100%;max-width:100%" alt="profile photo" src="data/myAvatar-2.png" class="hoverZoomLink"></a> -->	
            </td>	
          </tr>	
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
            <div style="max-height: 400px; overflow-y: scroll;">
            <ul>
            <li> <mark>(Oct 2023):</mark> Attended <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> and volunteered at the Google Booth. </li>
            <li> <mark>(Sep 2023):</mark> Five papers were accepted at <a href="https://www.agu.org/fall-meeting">AGU 2023</a>, including one oral presentation, one lightning talk, and three posters. [<a href="https://agu.confex.com/agu/fm23/prelim.cgi/Person/1357170">List 1</a>, <a href="https://agu.confex.com/agu/fm23/prelim.cgi/Person/1423415">List 2</a>].</li>
            <li> <mark>(June 2023):</mark> Our team's project is presented at Google I/O connect 2023 by <a href="https://in.linkedin.com/in/gauagg">Dr. Gaurav Aggarwal</a> [<a href="https://drive.google.com/file/d/1yeuWNe63wuZsLjhXglg0PIDqc4tEzLp8/view?t=728">Video</a>].</li>
            <li> <mark>(May 2023):</mark> Our team's project is selected as a demo in Google I/O 2023. [<a href="https://anthrokrishi-io-demo.uc.r.appspot.com/?lat=19.807014541714427&lng=75.77087495908933&zoom=17">Demo Website</a>].</li>
            <li> <mark>(April 2023):</mark> Our work on <a href="https://arxiv.org/pdf/2207.03075.pdf">Towards a Practical Utility of Federated Learning in the Medical Domain</a>  is accepted to <a href="https://www.chilconference.org/">CHIL 2023</a>. 
            <li> <mark>(Mar 2023):</mark> Gave a keynote talk on 'Towards Sustainable Agriculture prioritizing Global South using Machine Learning' in <a href="https://events.csa.iisc.ac.in/openday2023/#schedule">IISC open day 2023</a>.</li>
            <li> <mark>(Jan 2023):</mark> Our work on <a href="https://arxiv.org/pdf/2207.13083.pdf">TAPUDD: Task Agnostic and Post-hoc Unseen Distribution Detection</a> will be presented at WACV 2023 in Hawaii.</li>
            <li> <mark>(Dec 2022):</mark> Our team's project was highlighted at Google For India 2022 [<a href="https://youtu.be/lCFbbOgsm9I?t=1534">video</a>, <a href="https://blog.google/intl/en-in/company-news/inside-google/google-for-india-2022-ai-announcements/">G4I blog</a>].</li>
            <li> <mark>(Sep 2022):</mark> Joining <a href="https://research.google/teams/india-research-lab/">Google Research India</a> as a Predoctoral Researcher in the M2U2 team led by <a href="https://in.linkedin.com/in/gauagg">Dr. Gaurav Aggarwal</a>.</li>
            <li> <mark>(August 2022):</mark> I completed M.S. in AI from KAIST under the supervision of <a href="https://mp2893.com/">Prof. Edward Choi</a>.
            <li> <mark>(August 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2207.13083.pdf">TAPUDD: Task Agnostic and Post-hoc Unseen Distribution Detection</a>  is also accepted as a short paper to <a href="https://eccv22-arow.github.io">AROW Workshop</a>, ECCV 2022. 
            <li> <mark>(August 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2207.13083.pdf">TAPUDD: Task Agnostic and Post-hoc Unseen Distribution Detection</a> is accepted to <a href="https://wacv2023.thecvf.com/home">WACV 2023</a>. 
            <li> <mark>(August 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2208.13376.pdf">Reweighting Strategy based on Synthetic Data Identification for Sentence Similarity</a> is accepted to <a href="https://coling2022.org">COLING 2022</a>. 
            <li> <mark>(July 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2208.08853.pdf">Automatic Detection of Noisy Electrocardiogram Signals without Explicit Noise Labels</a> is accepted to <a href="https://sites.google.com/view/icpr-prha2022/home?authuser=0">PRHA Workshop</a>, ICPR 2022. 
            <li> <mark>(May 2022):</mark> Serving as a reviewer for <a href="https://eccv2022.ecva.net">ECCV 2022.
            <li> <mark>(Mar 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2201.07788.pdf">ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes</a> is accepted to CVPR 2022. 
            <li> <mark>(Apr 2021):</mark> Our work on <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.pdf">Beyond VQA: Generating Multi-word Answers and Rationales to Visual Questions</a> is accepted to <a href="https://mula-workshop.github.io/">MULA workshop, CVPR 2021</a>. 
            <li><mark>(Nov 2020):</mark> Serving as a Volunteer and Poster Mentor at Women in Machine Learning(WiML) workshop at NeurIPS 2020.</li>
            <li> <mark>(Sep 2020):</mark> I started my M.S. in the Graduate School of AI at <a href="https://www.kaist.ac.kr/en/">KAIST</a>. 
            <li> <mark>(Jun 2020):</mark> Serving as a volunteer for <a href="https://icml.cc/Conferences/2020">ICML 2020</a> and <a href="https://acl2020.org/">ACL 2020</a></li>
            <li> <mark>(Jun 2020):</mark> I started summer internship at <a href="https://www.brown.edu/">Brown University</a> under the supervision of <a href="http://srinathsridhar.com/">Prof. Srinath Sridhar.</a>. 
            <li> <mark>(Mar 2020):</mark> Serving as a Scholarship Application Reviewer</highlight> for the 2020 Grace Hopper Celebration (GHC 2020).</li>
            <li> <mark>(Aug 2019):</mark> Our paper <a href="https://ieeexplore.ieee.org/document/8969343">VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting</a> got accepted at <a href="http://www.2019.ieeeglobalsip.org/2019.ieeeglobalsip.org/index.html">GlobalSIP 2019</a>. Code is available on GitHub.</li>
            <li> <mark>(Jul 2019):</mark> Attended Summer School on Computer Vision 2019, CVIT, IIIT Hyderabad</li>
            <li> <mark>(May 2019):</mark> <a href="https://economictimes.indiatimes.com/etcampusstars/pasteditions?edition=2019">ET Campus Stars</a> by economic times, India's brightest engineers (2018-2019)</li>
            <li> <mark>(Nov 2018):</mark> Our team won second prize in <a href="http://www.celestiniprojectindia.com/">Celestini Project India</a> sponsored by Marconi Society and Google [News Coverage:<a href="https://www.financialexpress.com/industry/technology/delhi-pollution-indian-students-build-air-quality-measuring-app-air-congnizer/1373275/">Financial Express</a>
              
                    <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
              /
                    <a href="https://gadgets.ndtv.com/apps/news/delhi-air-pollution-college-students-develop-app-to-measure-air-quality-1943152">NDTV</a>
              /
                    <a href="https://www.business-standard.com/article/technology/indian-students-win-us-award-for-developing-mobile-app-to-monitor-aqi-level-118110500737_1.html">Business Standard</a>
              /
                    <a href="https://www.firstpost.com/tech/science/app-to-measures-pollution-levels-created-by-indian-students-wins-global-award-5505721.html">First Post</a>
              /
                    <a href="https://www.indiatoday.in/education-today/gk-current-affairs/story/air-quality-diwali-delhi-government-measures-1383399-2018-11-06">India Today</a>
              /
                    <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
              ].</li>
            <li> <mark>(Sep 2018):</mark> Received Grace Hopper Celebration India (GHCI) Student Scholarship.</li>
            <li> <mark>(Nov 2017):</mark> Our team won third prize in Hack Infinity.</li>
            <li> <mark>(Sep 2017):</mark> Our team won sixth position in India Hacks by Hackerearth.</li>
            <li> <mark>(Mar 2017):</mark> Our team won second prize in Hack In The North.</li>
            </ul>
          </div>
          </td></tr>
        </tbody></table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                          <tr>
                                          <td style="padding:20px;width:100%;vertical-align:middle">
                                            <heading>Publications</heading>
                                          </td>
                                        </tr>
                                      </tbody></table>
                                      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                      <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
                                        <tr>
                                          <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/condor.png" width="100%"></td>
                                          <td width="100%" valign="center">
                                          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                          <img src="data/NAS.png" alt="blind-date" width="280" height="250"></td> -->
                                          <!-- <td style="width:40%;vertical-align:middle"><img src="data/NAS.png" width="100%"></td> -->
                                          <!-- <td width="75%" valign="middle"> -->
                                          <!-- <td width="100%" height="100%"valign="center"> -->
                                              <papertitle>ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes.</papertitle>
                                            <br>
                                            <a href="https://scholar.google.com/citations?user=HAtfBjoAAAAJ&hl=en&oi=ao">Rahul Sajnani</a>, <a href="https://scholar.google.com/citations?hl=en&user=zsGbyGYAAAAJ">Adrien Poulenard</a>, <a href="https://scholar.google.com/citations?user=3Aa6D2oAAAAJ">Jivitesh Jain</a>, <strong>Radhika Dua, <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas, <a href="https://cs.brown.edu/people/ssrinath/">Srinath Sridhar</a>
                                            <br>
                                            <em>CVPR 2022 </em> 
                                            <br>
                                                      <a href="https://ivl.cs.brown.edu/ConDor/">Project Page</a>
                                                      /
                                                      <a href="https://arxiv.org/pdf/2201.07788.pdf">Paper</a>
                                                      /
                                                      <a href="https://www.youtube.com/watch?v=JKDiCvVPoSw">Video</a>
                                                      /
                                                      <a href="https://github.com/brown-ivl/ConDor">Code</a>
                                            <p> ConDor is a self-supervised method that learns to Canonicalize the 3D orientation and position for full and partial 3D point clouds. It can also learn to consistently co-segment object parts without any supervision.
                                            </p>
                                            </td>
                                         </tr>

                                        <tr>
                                          <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/NAS.png" width="100%"></td>
                                          <td width="100%" valign="center">
                                          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                          <img src="data/NAS.png" alt="blind-date" width="280" height="250"></td> -->
                                          <!-- <td style="width:40%;vertical-align:middle"><img src="data/NAS.png" width="100%"></td> -->
                                          <!-- <td width="75%" valign="middle"> -->
                                          <!-- <td width="100%" height="100%"valign="center"> -->
                                              <papertitle>Natural Attribute-based Shift Detection.</papertitle>
                                            <br>
                                            Jeonghoon Park<sup>*</sup>, Jimin Hong<sup>*</sup>, <strong>Radhika Dua<sup>*</sup></strong>, Daehoon Gwak, <a href="https://pages.cs.wisc.edu/~sharonli/">Yixuan Li</a>, <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Cho</a>, <a href="https://mp2893.com/">Edward Choi</a>
                                            <br>
                                            <em>Under Review </em> 
                                            <br>
                                                      <a href="https://arxiv.org/pdf/2110.09276.pdf">Paper</a>
                                                      <!-- /
                                                      <a href="data/MULA_11_CVPR21_ViQAR_slides.pdf">Slides</a>
                                                      /
                                                      <a href="data/MULA_11_CVPR21_ViQAR_video.mp4">Video</a>
                                                      /
                                                      <a href="data/MULA_11_CVPR21_ViQAR_poster.pdf">Poster</a> -->
                                            <p> We define a new task, natural attributebased shift (NAS) detection, to detect the samples shifted from the training distribution by some natural attribute such as age of subjects or brightness of images.
                                              We also introduce benchmark datasets in vision, language, and medical for NAS detection.
                                            </p>
                                            </td>
                                          </tr>

                                          

                                        <tr>
                                          <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_final.png" width="100%"></td>
                                          <td width="100%" valign="center">
                                          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                          <img src="data/viqar_final.png" alt="blind-date" width="280" height="250"></td> -->
                                        <!-- <td style="width:40%;max-width:100%;vertical-align:middle"><img src="data/viqar_final.png" width="100%"></td> -->
                                        <!-- <td width="75%" valign="middle"> -->
                                            <papertitle>Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions.</papertitle>
                                          <br>
                                          <strong>Radhika Dua<sup>*</sup></strong>, Sai Srinivas Kancheti<sup>*</sup>, <a href="https://iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a>
                                          <br>
                                          <em>MULA Workshop, CVPR </em> 2021
                                          <br>
                                                    <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.pdf">Paper</a>
                                                    /
                                                    <a href="data/MULA_11_CVPR21_ViQAR_slides.pdf">Slides</a>
                                                    /
                                                    <a href="data/MULA_11_CVPR21_ViQAR_video.mp4">Video</a>
                                                    /
                                                    <a href="data/MULA_11_CVPR21_ViQAR_poster.pdf">Poster</a>
                                          <p> We introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer.
                                          </p>
                                          </td>
                                        </tr>

                                      <tr>
                                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                                        <td width="100%" valign="center">
                                        <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                        <img src="data/vayu.png" alt="blind-date" width="280" height="250"></td> -->
                                        <!-- <td style="width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                                        <td width="100%" valign="center"> -->
                                        <!-- <td width="75%" valign="middle"> -->
                                                <papertitle>VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting</papertitle>
                                                <br>
                                                <strong>Radhika Dua<sup>*</sup></strong>, <a href="http://divyam3897.github.io/ ">Divyam Madaan<sup>*</sup></a>, <a href="http://www.iiits.ac.in/FacPages/index-prerana.html">Prerana Mukherjee</a>, <a href="https://web.iitd.ac.in/~brejesh/">Brejesh Lall</a>
                                                <br>
                                                <em>GlobalSIP</em>, 2019
                                                <br>
                                                  <a href="https://ieeexplore.ieee.org/document/8969343">Paper</a>
                                                  /
                                                  <a href="https://github.com/divyam3897/VayuAnukulani">Code</a>
                                                  /
                                                  <a href="https://sigport.org/sites/default/files/docs/VayuAnukulani_globalSip.pdf">slides</a>
                                               <p>We present VayuAnukulani system, a novel end-to-end solution to forecast fine-grained ambient air quality information based on the historical and realtime ambient air quality and meteorological data.
                                              </p>
                                            </td>
                                          </tr>
                                          </tbody></table>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Experience</heading>
                      </td>
                    </tr>
                  </tbody></table>
                  <table cellpadding="20" border="0" width="100%" align="center"><tbody>
                    <tr>
                      <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/Brown_University_Logo.png" width="60%"></td>
                      <td width="100%" valign="center">
                      <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/Brown_University_Logo.png"></td>
                      <td width="75%" valign="center"> -->
                          <papertitle>Summer Intern, Brown University</papertitle>
                        <br><strong>Supervisor:</strong> <a href="http://srinathsridhar.com/">Prof. Srinath Sridhar</a>
                          <p>
                          Conducting research in 3D computer vision and machine learning.
                        </p>
                      </td>
                    </tr>
                      <tr>
                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/iith.png" width="70%"></td>
                        <td width="100%" valign="center">
                        <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iith.png"></td>
                        <td width="75%" valign="center"> -->
                            <papertitle>Research Intern, Indian Institute of Technology, Hyderabad</papertitle>
                          <br><strong>Supervisor:</strong> <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                            <p>
                              Conducting research in Vision and Language applications and introduced a new task, ViQAR: Visual on
                              Answering and Reasoning, which focuses on automatic generation of the answer, and of a rationale,
                              given a visual query.
                          </p>
                        </td>
                      </tr>
                        <tr>
                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/iitd.jpeg" width="60%"></td>
                        <td width="100%" valign="center">
                        <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iitd.jpeg"></td> -->
                        <!-- <td valign="top" width="68%"> -->
                        <!-- <td width="75%" valign="center"> -->
                            <papertitle>Research Intern, Celestini Project India</papertitle>
                            <br><strong>Mentor:</strong> <a href="http://www.achowdhery.com/">Dr. Aakanksha Chowdhery</a> (Google Brain and Tensorflow) and <a href="http://web.iitd.ac.in/~brejesh/">Prof. Brejesh Lall</a> (IIT Delhi).
                            <br><strong>Sponsors:</strong> Marconi Society and Google
                            <p>
                              Developed a temporal forecasting solution based on the historical data reported by Central Pollution
                              Control board to predict the real-time and fine-grained air quality information in five locations of Delhi.
                            </p>
                        </td>
                      </tr>
                    </tbody></table>


                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Teaching</heading>
                      </td>
                    </tr>
                  <table cellpadding="20" border="0" width="100%" align="center">
                    <tbody><tr>
                      <!-- <td style="padding:40px;vertical-align:middle"><img src="data/artificial-intelligence.jpg"></td> -->
                      <!-- <td width="40%"><img src="data/artificial-intelligence.jpg"  style="padding:20px;vertical-align:middle;border-radius:15px" width="80%"></td>
                      <td width="80%" valign="center"> -->
                      <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/artificial-intelligence.jpg" width="100%"></td>
                      <td width="100%" valign="center">
                      <a href="https://dl4cv-nptel.github.io/DL4CVBK/intro.html"><papertitle>NPTEL: Deep Learning for Computer Vision</papertitle></a><br>
                        <strong>Instructor</strong>: <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                      <br>
                      <br>
                      <a href="https://docs.google.com/document/d/17cwez2X6zGBo4K25Iyy1vyqeXU5jtsso0Pu7mvF_HJ4/edit"><papertitle>IIT Hyderabad-CS5370CS6360: Advanced Topics in
                        Machine Learning - Spring 2020</papertitle></a><br>
                        <strong>Instructor</strong>: <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                      <br>
                      <br>
                      <a href="https://docs.google.com/document/d/19AyKxnrUFfC3vK0yv8safTyRG2apicTrAp8F1tkMVjg/edit"><papertitle>IIT Hyderabad-CS5370: Deep Learning for Computer Vision - Fall 2019 </papertitle></a><br>
                      <strong>Instructor</strong>: <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                      <br>
                      <br>
                      <a href="https://docs.google.com/document/d/19AyKxnrUFfC3vK0yv8safTyRG2apicTrAp8F1tkMVjg/edit"><papertitle>KAIST-CC500: Scientific Writing</papertitle></a><br>
                      <strong>Instructor</strong>: <a href="https://efl.kaist.ac.kr/research/view/vid/28">Prof. Mik Fanguy</a>
                      <br>
                      <br>
                      <a href="https://docs.google.com/document/d/19AyKxnrUFfC3vK0yv8safTyRG2apicTrAp8F1tkMVjg/edit"><papertitle>KAIST-HSS583: Graduate English Presentations</papertitle></a><br>
                      <strong>Instructor</strong>: <a href="https://efl.kaist.ac.kr/research/view/vid/28">Prof. Mik Fanguy</a>
                      <br>
                      </td>
                    </tr>
                  </tbody></table>


                  <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody>
                      <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle"><img src="data/iith.png"></td>
                        <td width="80%" valign="center"> -->



                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Selected Projects</heading>
                      </td>
                    </tr>
                  </tbody></table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
                      <tr>
                    <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td>
                    <td width="100%" valign="center">
                        <papertitle>ViQAR: Visual Question Answering and Reasoning</papertitle>
                      <br>
                      <strong>Radhika Dua</strong>, Sai Srinivas Kancheti, <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                      <p> We introduced a new task, ViQAR, in which the model generates the complete answer and rationale. We also proposed an end-to-end, attention-based
encoder-decoder architecture to solve this task, and showed that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as human Turing Test.
                      </p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;vertical-align:middle">
                          <!-- <img src="images/B3DO.jpg" alt="b3do" width="160" style="border-style: none"> -->
                          <iframe  width=100% height=100% preload="auto" id='nerf_image' muted autoplay loop src="https://www.youtube.com/embed/l7-dK4eKxG0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </td>
                        <td width="100%" valign="center">
                          <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                            <papertitle>Clair: Air Pollution Prediction </papertitle>
                            <br>
                            <strong>Radhika Dua</strong>, <a href="http://divyam3897.github.io/ ">Divyam Madaan</a>, <a href="http://www.achowdhery.com/">Dr. Aakanksha Chowdhery</a>
                          <p>As a part of the Celestini Project, sponsored by Google and Marconi Society and mentored by Dr. Aakanksha Chowdhery, we developed Clair: Air pollution forecasting in Delhi, a temporal forecasting solution based on the historical data reported by Central Pollution
                          Control board to predict the real-time and fine-grained air quality information in five locations of Delhi. We were awarded Second Prize for our novel solution and also had the opportunity of presenting our work at GlobalSIP 2019.
                                <br>
                                <a href="https://marconiyoungscholars.org/sparking-young-minds-to-solve-air-pollution-and-traffic-fatality-problems/">website</a>
                          /
                                <a href="https://www.youtube.com/watch?v=l7-dK4eKxG0">video</a>
                          /
                                <a href="https://www.financialexpress.com/industry/technology/delhi-pollution-indian-students-build-air-quality-measuring-app-air-congnizer/1373275/">Financial Express</a>
                          /
                                <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
                          /
                                <a href="https://gadgets.ndtv.com/apps/news/delhi-air-pollution-college-students-develop-app-to-measure-air-quality-1943152">NDTV</a>
                          /
                                <a href="https://www.business-standard.com/article/technology/indian-students-win-us-award-for-developing-mobile-app-to-monitor-aqi-level-118110500737_1.html">Business Standard</a>
                          /
                                <a href="https://www.firstpost.com/tech/science/app-to-measures-pollution-levels-created-by-indian-students-wins-global-award-5505721.html">First Post</a>
                          /
                                <a href="https://www.indiatoday.in/education-today/gk-current-affairs/story/air-quality-diwali-delhi-government-measures-1383399-2018-11-06">India Today</a>
                          /
                                <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
                          /
                          <a href="https://github.com/divyam3897/VayuAnukulani">code</a>
                          </p>
                        </td>
                      </tr>
                      </tbody></table>
                        <br>
                        Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
</html>
