<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Radhika Dua</title>

  <meta name="author" content="Radhika Dua">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Radhika Dua</name>
              </p>
              <p>Hi, I am a Master's student in Graduate School of AI at Korea Advanced Institute of Science and Technology (KAIST). I am fortunate to have <a href="https://mp2893.com/">Prof. Edward Choi</a> as my advisor. 
                My research interest lies at the intersection of Computer Vision, Machine Learning, Healthcare, and NLP. I currently work on enhancing the reliability of deep neural networks against natural and synthetic distribution shifts.
                <!-- I am interested in Artificial Intelligence at the intersection of Computer Vision, Machine Learning & Healthcare. I am currently working on out-of-distribution detection in deep learning. -->
              <br>
              <br>
              Before joining KAIST, I was a Summer Intern at <a href="https://www.brown.edu/">Brown University</a>, where I was fortunate to be advised by <a href="https://ai.stanford.edu/~ssrinath/">Prof. Srinath Sridhar</a>. Prior to my time at Brown University, I was a Visiting Researcher at <a href="https://iith.ac.in/">IIT Hyderabad</a>, where I was blessed to be supervised by <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>. 
              <p style="text-align:center">
                <a href="mailto:radhikadua@kaist.ac.kr">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=SeBAKsUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Radhikadua123">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/dua_radhika?lang=en">Twitter</a>
              </p>
            </td>
            <td style="width:100%;max-width:100%">
              <a href="data/rd4.jpeg"><img style="border-radius:100%;width:90%;height:35%;max-width:100%" alt="profile photo" src="data/rd4.jpeg" class="hoverZoomLink"></a>
              <!-- <a href="data/myAvatar.png"><img style="border-radius:48%;width:100%;max-width:100%" alt="profile photo" src="data/myAvatar-2.png" class="hoverZoomLink"></a> -->
            </td>
          </tr>
        </tbody></table>
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>RESEARCH</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, depth, motion, paint, light, colors, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerf_image'>
                  <embed width=100% height=100%  muted autoplay loop src="https://www.youtube.com/embed/tgbNymZ7vqY">
  <!-- <video  width=100% height=100% preload="none" muted autoplay loop>
                  <source type="video/youtube" src="https://www.youtube.com/watch?v=l7-dK4eKxG0#action=share" />
  </video> -->
<!-- </div>
                <img src='data/vase_still.png' width="160">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/nerf">
                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan*</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
        <em>arXiv</em>, 2020
              <br>
              <a href="http://www.matthewtancik.com/nerf">project page</a>
        /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">video</a>
        /
              <a href="https://github.com/bmild/nerf">code</a>
              <p></p>
              <p>
              Training a tiny non-convolutional neural network to reproduce a scene using volume rendering achieves photorealistic view synthesis.</p> -->
            <!-- </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/B3DO.jpg" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle"> -->
              <!-- <a href=""> -->
                <!-- <papertitle> Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions</papertitle> -->
              <!-- </a> -->
              <!-- <br>
              <a href="http://www.eecs.berkeley.edu/%7Eallie/">Allison Janoch</a>,
              <a href="http://sergeykarayev.com/">Sergey Karayev</a>,
              <a href="http://www.eecs.berkeley.edu/%7Ejiayq/">Yangqing Jia</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://www.cs.berkeley.edu/%7Emfritz/">Mario Fritz</a>,
              <a href="http://www.icsi.berkeley.edu/%7Esaenko/">Kate Saenko</a>,
              <a href="http://www.eecs.berkeley.edu/%7Etrevor/">Trevor Darrell</a>
              <br>
              <em>Under Review</em>,
              <br> -->
              <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
              <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a>
              <p>We present a large RGB-D dataset of indoor scenes and investigate ways to improve object detection using depth information.</p>
            </td>
          </tr>
                  </tbody></table>  -->

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
                      <tr>
                        <td>
                          <heading>News</heading>
                        <ul>
                        <li> <mark>(Mar 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2201.07788.pdf">ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes</a> is accepted to CVPR 2022. 
                        <li> <mark>(Apr 2021):</mark> Our work on <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.pdf">Beyond VQA: Generating Multi-word Answers and Rationales to Visual Questions</a> is accepted to <a href="https://mula-workshop.github.io/">MULA workshop, CVPR 2021</a>. 
                        <li><mark>(Nov 2020):</mark>Serving as a Volunteer and Poster Mentor at Women in Machine Learning(WiML) workshop at NeurIPS 2020.</li>
                        <li> <mark>(Sep 2020):</mark> I started my M.S. in the Graduate School of AI at <a href="https://www.kaist.ac.kr/en/">KAIST</a>. 
                        <li> <mark>(Jun 2020):</mark> Serving as a volunteer for <a href="https://icml.cc/Conferences/2020">ICML 2020</a> and <a href="https://acl2020.org/">ACL 2020</a></li>
                        <li> <mark>(Jun 2020):</mark> I started summer internship at <a href="https://www.brown.edu/">Brown University</a> under the supervision of <a href="http://srinathsridhar.com/">Prof. Srinath Sridhar</a>. 
                        <li> <mark>(Mar 2020):</mark> Serving as a Scholarship Application Reviewer</highlight> for the 2020 Grace Hopper Celebration (GHC 2020)</li>
                        <li> <mark>(Aug 2019):</mark> Our paper <a href="https://ieeexplore.ieee.org/document/8969343">VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting</a> got accepted at <a href="http://www.2019.ieeeglobalsip.org/2019.ieeeglobalsip.org/index.html">GlobalSIP 2019</a>. Code is available on GitHub.</li>
                        <li> <mark>(Jul 2019):</mark> Attended Summer School on Computer Vision 2019, CVIT, IIIT Hyderabad</li>
                        <li> <mark>(May 2019):</mark> <a href="https://economictimes.indiatimes.com/etcampusstars/pasteditions?edition=2019">ET Campus Stars</a> by economic times, India's brightest engineers (2018-2019)</li>
                        <li> <mark>(Nov 2018):</mark> Our team won second prize in <a href="http://www.celestiniprojectindia.com/">Celestini Project India</a> sponsored by Marconi Society and Google [News Coverage:<a href="https://www.financialexpress.com/industry/technology/delhi-pollution-indian-students-build-air-quality-measuring-app-air-congnizer/1373275/">Financial Express</a>
                          
                                <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
                          /
                                <a href="https://gadgets.ndtv.com/apps/news/delhi-air-pollution-college-students-develop-app-to-measure-air-quality-1943152">NDTV</a>
                          /
                                <a href="https://www.business-standard.com/article/technology/indian-students-win-us-award-for-developing-mobile-app-to-monitor-aqi-level-118110500737_1.html">Business Standard</a>
                          /
                                <a href="https://www.firstpost.com/tech/science/app-to-measures-pollution-levels-created-by-indian-students-wins-global-award-5505721.html">First Post</a>
                          /
                                <a href="https://www.indiatoday.in/education-today/gk-current-affairs/story/air-quality-diwali-delhi-government-measures-1383399-2018-11-06">India Today</a>
                          /
                                <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
                          ]</li>
                        <li> <mark>(Sep 2018):</mark> Received Grace Hopper Celebration India (GHCI) Student Scholarship </li>
                        <li> <mark>(Nov 2017):</mark> Our team won third prize in Hack Infinity </li>
                        <li> <mark>(Sep 2017):</mark> Our team won sixth position in India Hacks by Hackerearth</li>
                        <li> <mark>(Mar 2017):</mark> Our team won second prize in Hack In The North</li>
                        </ul>
                      </td></tr>
                    </tbody></table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                          <tr>
                                          <td style="padding:20px;width:100%;vertical-align:middle">
                                            <heading>PUBLICATIONS</heading>
                                          </td>
                                        </tr>
                                      </tbody></table>
                                      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                      <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
                                        <tr>
                                          <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/NAS.png" width="100%"></td>
                                          <td width="100%" valign="center">
                                          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                          <img src="data/NAS.png" alt="blind-date" width="280" height="250"></td> -->
                                          <!-- <td style="width:40%;vertical-align:middle"><img src="data/NAS.png" width="100%"></td> -->
                                          <!-- <td width="75%" valign="middle"> -->
                                          <!-- <td width="100%" height="100%"valign="center"> -->
                                              <papertitle>Natural Attribute-based Shift Detection.</papertitle>
                                            <br>
                                            Jeonghoon Park<sup>*</sup>, Jimin Hong<sup>*</sup>, <strong>Radhika Dua<sup>*</sup></strong>, Daehoon Gwak, <a href="https://pages.cs.wisc.edu/~sharonli/">Yixuan Li</a>, <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Cho</a>, <a href="https://mp2893.com/">Edward Choi</a>
                                            <br>
                                            <em>Under Review </em> 
                                            <br>
                                                      <a href="https://arxiv.org/pdf/2110.09276.pdf">Paper</a>
                                                      <!-- /
                                                      <a href="data/MULA_11_CVPR21_ViQAR_slides.pdf">Slides</a>
                                                      /
                                                      <a href="data/MULA_11_CVPR21_ViQAR_video.mp4">Video</a>
                                                      /
                                                      <a href="data/MULA_11_CVPR21_ViQAR_poster.pdf">Poster</a> -->
                                            <p> We define a new task, natural attributebased shift (NAS) detection, to detect the samples shifted from the training distribution by some natural attribute such as age of subjects or brightness of images.
                                              We also introduce benchmark datasets in vision, language, and medical for NAS detection.
                                            </p>
                                            </td>
                                          </tr>

                                        <tr>
                                          <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_final.png" width="100%"></td>
                                          <td width="100%" valign="center">
                                          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                          <img src="data/viqar_final.png" alt="blind-date" width="280" height="250"></td> -->
                                        <!-- <td style="width:40%;max-width:100%;vertical-align:middle"><img src="data/viqar_final.png" width="100%"></td> -->
                                        <!-- <td width="75%" valign="middle"> -->
                                            <papertitle>Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions.</papertitle>
                                          <br>
                                          <strong>Radhika Dua<sup>*</sup></strong>, Sai Srinivas Kancheti<sup>*</sup>, <a href="https://iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a>
                                          <br>
                                          <em>MULA Workshop, CVPR </em> 2021
                                          <br>
                                                    <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.pdf">Paper</a>
                                                    /
                                                    <a href="data/MULA_11_CVPR21_ViQAR_slides.pdf">Slides</a>
                                                    /
                                                    <a href="data/MULA_11_CVPR21_ViQAR_video.mp4">Video</a>
                                                    /
                                                    <a href="data/MULA_11_CVPR21_ViQAR_poster.pdf">Poster</a>
                                          <p> We introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer.
                                          </p>
                                          </td>
                                        </tr>

                                      <tr>
                                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                                        <td width="100%" valign="center">
                                        <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                        <img src="data/vayu.png" alt="blind-date" width="280" height="250"></td> -->
                                        <!-- <td style="width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                                        <td width="100%" valign="center"> -->
                                        <!-- <td width="75%" valign="middle"> -->
                                                <papertitle>VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting</papertitle>
                                                <br>
                                                <strong>Radhika Dua<sup>*</sup></strong>, <a href="http://divyam3897.github.io/ ">Divyam Madaan<sup>*</sup></a>, <a href="http://www.iiits.ac.in/FacPages/index-prerana.html">Prerana Mukherjee</a>, <a href="https://web.iitd.ac.in/~brejesh/">Brejesh Lall</a>
                                                <br>
                                                <em>GlobalSIP</em>, 2019
                                                <br>
                                                  <a href="https://ieeexplore.ieee.org/document/8969343">Paper</a>
                                                  /
                                                  <a href="https://github.com/divyam3897/VayuAnukulani">Code</a>
                                                  /
                                                  <a href="https://sigport.org/sites/default/files/docs/VayuAnukulani_globalSip.pdf">slides</a>
                                               <p>We present VayuAnukulani system, a novel end-to-end solution to forecast fine-grained ambient air quality information based on the historical and realtime ambient air quality and meteorological data.
                                              </p>
                                            </td>
                                          </tr>
                                          </tbody></table>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>EXPERIENCE</heading>
                      </td>
                    </tr>
                  </tbody></table>
                  <table cellpadding="20" border="0" width="100%" align="center"><tbody>
                    <tr>
                      <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/Brown_University_Logo.png" width="60%"></td>
                      <td width="100%" valign="center">
                      <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/Brown_University_Logo.png"></td>
                      <td width="75%" valign="center"> -->
                          <papertitle>Summer Intern, Brown University</papertitle>
                        <br><strong>Supervisor:</strong> <a href="http://srinathsridhar.com/">Prof. Srinath Sridhar</a>
                          <p>
                          Conducting research in 3D computer vision and machine learning.
                        </p>
                      </td>
                    </tr>
                      <tr>
                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/iith.png" width="70%"></td>
                        <td width="100%" valign="center">
                        <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iith.png"></td>
                        <td width="75%" valign="center"> -->
                            <papertitle>Research Intern, Indian Institute of Technology, Hyderabad</papertitle>
                          <br><strong>Supervisor:</strong> <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                            <p>
                              Conducting research in Vision and Language applications and introduced a new task, ViQAR: Visual on
                              Answering and Reasoning, which focuses on automatic generation of the answer, and of a rationale,
                              given a visual query.
                          </p>
                        </td>
                      </tr>
                        <tr>
                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/iitd.jpeg" width="60%"></td>
                        <td width="100%" valign="center">
                        <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iitd.jpeg"></td> -->
                        <!-- <td valign="top" width="68%"> -->
                        <!-- <td width="75%" valign="center"> -->
                            <papertitle>Research Intern, Celestini Project India</papertitle>
                            <br><strong>Mentor:</strong> <a href="http://www.achowdhery.com/">Dr. Aakanksha Chowdhery</a> (Google Brain and Tensorflow) and <a href="http://web.iitd.ac.in/~brejesh/">Prof. Brejesh Lall</a> (IIT Delhi).
                            <br><strong>Sponsors:</strong>Marconi Society and Google
                            <p>
                              Developed a temporal forecasting solution based on the historical data reported by Central Pollution
                              Control board to predict the real-time and fine-grained air quality information in five locations of Delhi.
                            </p>
                        </td>
                      </tr>
                    </tbody></table>


                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>TEACHING</heading>
                      </td>
                    </tr>
                  <table cellpadding="20" border="0" width="100%" align="center">
                    <tbody><tr>
                      <!-- <td style="padding:40px;vertical-align:middle"><img src="data/artificial-intelligence.jpg"></td> -->
                      <!-- <td width="40%"><img src="data/artificial-intelligence.jpg"  style="padding:20px;vertical-align:middle;border-radius:15px" width="80%"></td>
                      <td width="80%" valign="center"> -->
                      <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/artificial-intelligence.jpg" width="100%"></td>
                      <td width="100%" valign="center">
                      <a href="https://docs.google.com/document/d/17cwez2X6zGBo4K25Iyy1vyqeXU5jtsso0Pu7mvF_HJ4/edit"><papertitle>CS6360: Advanced Topics in
                        Machine Learning - Spring 2020</papertitle></a><br>
                        <strong>Instructor</strong>: Prof. Vineeth N Balasubramanian
                      <br>
                      <br>
                      <a href="https://docs.google.com/document/d/19AyKxnrUFfC3vK0yv8safTyRG2apicTrAp8F1tkMVjg/edit"><papertitle>CS5370: Deep Learning for Computer Vision - Fall 2019 </papertitle></a><br>
                      <strong>Instructor</strong>: Prof. Vineeth N Balasubramanian
                      <br>
                      </td>
                    </tr>
                  </tbody></table>


                  <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody>
                      <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle"><img src="data/iith.png"></td>
                        <td width="80%" valign="center"> -->



                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>SELECTED PROJECTS</heading>
                      </td>
                    </tr>
                  </tbody></table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
                      <tr>
                    <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td>
                    <td width="100%" valign="center">
                        <papertitle>ViQAR: Visual Question Answering and Reasoning</papertitle>
                      <br>
                      <strong>Radhika Dua</strong>, Sai Srinivas Kancheti, <a href="https://iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a>
                      <p> We introduced a new task, ViQAR, in which the model generates the complete answer and rationale. We also proposed an end-to-end, attention-based
encoder-decoder architecture to solve this task, and showed that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as human Turing Test.
                      </p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;vertical-align:middle">
                          <!-- <img src="images/B3DO.jpg" alt="b3do" width="160" style="border-style: none"> -->
                          <iframe  width=100% height=100% preload="auto" id='nerf_image' muted autoplay loop src="https://www.youtube.com/embed/l7-dK4eKxG0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </td>
                        <td width="100%" valign="center">
                          <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                            <papertitle>Clair: Air Pollution Prediction </papertitle>
                            <br>
                            <strong>Radhika Dua</strong>, <a href="http://divyam3897.github.io/ ">Divyam Madaan</a>, <a href="http://www.achowdhery.com/">Dr. Aakanksha Chowdhery</a>
                          <p>As a part of the Celestini Project, sponsored by Google and Marconi Society and mentored by Dr. Aakanksha Chowdhery, we developed Clair: Air pollution forecasting in Delhi, a temporal forecasting solution based on the historical data reported by Central Pollution
                          Control board to predict the real-time and fine-grained air quality information in five locations of Delhi. We were awarded Second Prize for our novel solution and also had the opportunity of presenting our work at GlobalSIP 2019.
                                <br>
                                <a href="https://marconiyoungscholars.org/sparking-young-minds-to-solve-air-pollution-and-traffic-fatality-problems/">website</a>
                          /
                                <a href="https://www.youtube.com/watch?v=l7-dK4eKxG0">video</a>
                          /
                                <a href="https://www.financialexpress.com/industry/technology/delhi-pollution-indian-students-build-air-quality-measuring-app-air-congnizer/1373275/">Financial Express</a>
                          /
                                <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
                          /
                                <a href="https://gadgets.ndtv.com/apps/news/delhi-air-pollution-college-students-develop-app-to-measure-air-quality-1943152">NDTV</a>
                          /
                                <a href="https://www.business-standard.com/article/technology/indian-students-win-us-award-for-developing-mobile-app-to-monitor-aqi-level-118110500737_1.html">Business Standard</a>
                          /
                                <a href="https://www.firstpost.com/tech/science/app-to-measures-pollution-levels-created-by-indian-students-wins-global-award-5505721.html">First Post</a>
                          /
                                <a href="https://www.indiatoday.in/education-today/gk-current-affairs/story/air-quality-diwali-delhi-government-measures-1383399-2018-11-06">India Today</a>
                          /
                                <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
                          /
                          <a href="https://github.com/divyam3897/VayuAnukulani">code</a>
                          </p>
                        </td>
                      </tr>
                      </tbody></table>
                        <br>
                        Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
</html>
